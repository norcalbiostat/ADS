---
title: "Week 12: Resampling"
date: "`r paste(Sys.Date(), emo::ji('balloon'), emo::ji('birthday'), emo::ji('tada'), emo::ji('gift'))`"
output:   
  html_document: 
    highlight: tango
    theme: yeti
    toc: yes
    toc_float: true
---

# Learning Path

### Where we've been

* Multivariable model building using linear and logistic regression. 
* Basic concepts surrounding flexible vs ridgid, parametric vs non-parametric estimates of $f$. 
* Basic concepts of prediction and classification. Target measures of fit. 
* Wrangling and exploring text data
* Classification models other than logistic regression. Starting simple with discrimenant analysis and naieve bayes. (ISLR Ch 4)

### Where we're at

* Employing resampling methods such as cross validation to improve our prediction accuracy (ISLR Ch 5)

### Where we're going

* Topic modeling of text data - making predictions using text. 
* Tree based methods for classification (ISLR Ch 8)


----

# Goals for the week

* Learn about a key component to model optimization. 
    - Theoretical learning and discussion in class
    - Applied learning as part of the homework. 

----

# Housekeeping 

* Project updates - Missing for about half of you. 
* Update on homework. 
    - Classification homework Do ISLR Ch 4 # 10a-h instead. If you've already started 13, go ahead and stick with it. Just don't get too deep into the weeds of variable selection. 
    - Resampling. Answer questions that are in this set of notes. This week: Go get a data set. 
    - If you haven't copied the homework repo yet, the repo has been updated. 
* Continuing with the in class shared student-centered learning. We'll go through these notes at a measured pace. The videos are longer, so we may not finish both sections today. I'm not in a rush, we'll wrap this into next week if needed. 

----

# Learning - Theory

`r emo::ji("busts_in_silhouette")` Discuss these questions in your group and write the answers in the `resampling-assignment.Rmd` file in your newest homework repo for this week. You may not finish all questions in the time allotted during class, you will have to finish outside of class. 

### Introduction and the validation set approach. (ISLR 5.1.1) [[Video  (14:01)]](https://www.youtube.com/watch?v=_2ij6eaaSl0&list=PL5-da3qGB5IA6E6ZNXu7dp89_uv8yocmf)

1. What are the two most commonly used resampling methods? 
2. The process of evaluating a modelâ€™s performance is known as ____. 
3. The process of selecting the proper level of flexibility for a model is known as _____. 
4. The training error is typically ______ estimates the test error rate. 
5. What is the main reason to use cross-validation? 
6. What is the main reason to use bootstrap? 
7. As the model complexity increases, what happens to the training and testing errors? 
8. Why is the bias low for models with high complexity? 
9. What is a validation set? 
10. Describe the validation process. 
11. What are the drawbacks of the validation set approach? 


### Cross-Validation and LOOCV for Regression and Classification (ISLR 5.1.2- 5.1.5) [[Video (13:33)]](https://www.youtube.com/watch?v=nZAM5OXrktY&list=PL5-da3qGB5IA6E6ZNXu7dp89_uv8yocmf&index=2)

_The video describes K-fold first, then LOOCV second. This is opposite of the order in the book._

#### Cross-Validation (5.1.2)
1. Describe the process of K-fold cross validation. 
2. _In English_  how do you calculate the cross-validation error rate? 

#### LOOCF (5.1.3)
3. How is LOOCF a special case of CV? 
4. What is a drawback of LOOCF when $n$ is large? 
5. When would we only care about the location of the minimum point in the test MSE curve, and not the actual value of the minimum MSE itself? 


#### Bias-Variance trade-off for k-Fold CV (5.1.4)
6. Why does k-fold CV give more accurate estimates of the test error rate than LOOCF? Explain this in english. 
7. What is a good value for $k$ that will result in test error rates that find a nice balance between bias and variance? 


### The Boostrap (5.2) [[Video (11:29)]](https://www.youtube.com/watch?v=p4BYWX7PTBM&list=PL5-da3qGB5IA6E6ZNXu7dp89_uv8yocmf&index=4)


> in progress

----

# Learning - Application

We will use the `caret` package for most, if not all, of our machine learning algorithms. [Here is a link to the vignette/book](http://topepo.github.io/caret/index.html). 

Of particular note  - Chapter 3. Real data is not clean. Real data cannot be shoved into a model without pre-processing. Mindful pre-processing. 


> in progress


